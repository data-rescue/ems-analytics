{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "golden-blair",
   "metadata": {},
   "source": [
    "## Raw Datasets From Excel to Parquet\n",
    "\n",
    "The idea of this notebook is to create parquet files of each dataset to improve I/O and storage speed of each dataset.\n",
    "\n",
    "#### Importing Key Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fifth-controversy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-consumer",
   "metadata": {},
   "source": [
    "#### Reading Raw Data\n",
    "We are using version 4 of the raw dataset provided by the Fairfax Fire and Rescue Department on February 25, 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "needed-throw",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pat = pd.read_excel(io = '../data/01_raw/20210225-ems-raw-v04.xlsx', sheet_name='Patients')\n",
    "df_pro = pd.read_excel(io = '../data/01_raw/20210225-ems-raw-v04.xlsx', sheet_name='Procedures')\n",
    "df_med = pd.read_excel(io = '../data/01_raw/20210225-ems-raw-v04.xlsx', sheet_name='Medications')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-johns",
   "metadata": {},
   "source": [
    "#### Adjusting Value Type for Each Series in the Data Frame.\n",
    "In this section we are considering that:\n",
    "* all ID attributes will be considered as strings\n",
    "* all Date attributes will be set to datetime64\n",
    "* Any other type of attributes will be considered as categorical\n",
    "\n",
    "Setting values as categorical as much as possible and where appropriate will help improving performance during analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "developmental-asbestos",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pat = df_pat.astype(dtype = {'Shift': 'category',\n",
    "                                'FireStation': 'category',\n",
    "                                'Battalion': 'category',\n",
    "                                'PatientOutcome': 'category',\n",
    "                                'PatientGender': 'category',\n",
    "                                'FRDPersonnelGender': 'category',\n",
    "                                'UnitId': 'category',              \n",
    "                                'CrewMemberRoles': 'category',     \n",
    "                                'PatientId': 'string',\n",
    "                                'FRDPersonnelID': 'string',\n",
    "                                'DispatchTime': 'datetime64',\n",
    "                                'FRDPersonnelStartDate': 'datetime64'})\n",
    "\n",
    "df_pro = df_pro.astype(dtype = {'Procedure_Performed_Code': 'category',\n",
    "                                'Procedure_Performed_Description': 'category',\n",
    "                                'PatientId': 'string',\n",
    "                                'FRDPersonnelID': 'string',\n",
    "                                'Dim_Procedure_PK': 'string',\n",
    "                                'Procedure_Performed_Date_Time': 'datetime64'})\n",
    "\n",
    "df_med = df_med.astype(dtype = {'Medication_Given_RXCUI_Code': 'category',\n",
    "                                'Medication_Given_Description': 'category',\n",
    "                                'PatientId': 'string',\n",
    "                                'FRDPersonnelID': 'string',\n",
    "                                'Dim_Medication_PK': 'string',\n",
    "                                'Medication_Administered_Date_Time': 'datetime64'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-female",
   "metadata": {},
   "source": [
    "#### Writing Parquet Files into Local Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "opening-petroleum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Convert DataFrame into PyArrow Table\n",
    "pat_tab_w = pa.Table.from_pandas(df_pat)\n",
    "pro_tab_w = pa.Table.from_pandas(df_pro)\n",
    "med_tab_w = pa.Table.from_pandas(df_med)\n",
    "\n",
    "# Step 2: Write the Table into parquet format and save it in the raw data folder.\n",
    "pq.write_table(pat_tab_w, '../data/01_raw/20210225-ems-v04-patients-raw.parquet')\n",
    "pq.write_table(pro_tab_w, '../data/01_raw/20210225-ems-v04-procedures-raw.parquet')\n",
    "pq.write_table(med_tab_w, '../data/01_raw/20210225-ems-v04-medications-raw.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-argentina",
   "metadata": {},
   "source": [
    "#### Creating New DataFrames from Parquet\n",
    "The following script can be repeated in any of your notebooks to load the DataFrames from parquet instead of the excel file. If you are like me that creates a new notebook for each concern, you will enjoy of faster loading time than using excel files.\n",
    "\n",
    "For illustration purposes, I have included the libraries you need to import for this script to work as intended on any notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mobile-therapist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "pat_df_r = pq.read_table('../data/01_raw/20210225-ems-v04-patients-raw.parquet')\\\n",
    "             .to_pandas(categories=['FireStation','Battalion'])\n",
    "pro_df_r = pq.read_table('../data/01_raw/20210225-ems-v04-procedures-raw.parquet')\\\n",
    "             .to_pandas(categories=['Procedure_Performed_Code'])\n",
    "med_df_r = pq.read_table('../data/01_raw/20210225-ems-v04-medications-raw.parquet')\\\n",
    "             .to_pandas(categories=['Medication_Given_RXCUI_Code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-panel",
   "metadata": {},
   "source": [
    "#### Excel to Parquet DataFrames Validation\n",
    "As a validation mechanisim, I'm going to run a test for each dataset to validate that both data frames present the same shape. This should give you the certainty that no records were lost during after converting the files to parquet.\n",
    "\n",
    "Have in mind that we also adjusted the data types to ensure a better performance by Pandas and consistency across notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "numerical-effects",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASS: DataFrame integrity maintained for the Patients dataset.\n"
     ]
    }
   ],
   "source": [
    "assert df_pat.shape == pat_df_r.shape, 'FAIL: DataFrame shapes are not the same for Patients dataset.'\n",
    "print ('PASS: DataFrame integrity maintained for the Patients dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "anticipated-kingston",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASS: DataFrame integrity maintained for the Procedures dataset.\n"
     ]
    }
   ],
   "source": [
    "assert df_pro.shape == pro_df_r.shape, 'FAIL: DataFrame shapes are not the same for Procedures dataset.'\n",
    "print ('PASS: DataFrame integrity maintained for the Procedures dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "forced-sunrise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASS: DataFrame integrity maintained for the Medications dataset.\n"
     ]
    }
   ],
   "source": [
    "assert df_med.shape == med_df_r.shape, 'FAIL: DataFrame shapes are not the same for Medications dataset.'\n",
    "print ('PASS: DataFrame integrity maintained for the Medications dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-confidence",
   "metadata": {},
   "source": [
    "#### Uploading New Dataset Objects to S3\n",
    "Since this is an infrequent activity I have imported the local library in this section as well. However, you will not receive the same results because the objects already exist in S3 or you don't have access to the S3 bucket by the time you may try to run this portion of the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "champion-journalism",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 20210225-ems-v04-patients-raw.parquet file already exist in the S3 bucket under the key 01_raw/20210225-ems-v04-patients-raw.parquet\n",
      "The 20210225-ems-v04-procedures-raw.parquet file already exist in the S3 bucket under the key 01_raw/20210225-ems-v04-procedures-raw.parquet\n",
      "The 20210225-ems-v04-medications-raw.parquet file already exist in the S3 bucket under the key 01_raw/20210225-ems-v04-medications-raw.parquet\n"
     ]
    }
   ],
   "source": [
    "from src.d01_data.ingest import ProjectIngest\n",
    "\n",
    "ProjectIngest('01_raw','20210225-ems-v04-patients-raw.parquet').remote_upload()\n",
    "ProjectIngest('01_raw','20210225-ems-v04-procedures-raw.parquet').remote_upload()\n",
    "ProjectIngest('01_raw','20210225-ems-v04-medications-raw.parquet').remote_upload()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
